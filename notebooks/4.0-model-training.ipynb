{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c1788f",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908e5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a33c31",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d82cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"json\", data_files='../data/invoice_ner_dataset.jsonl')['train']\n",
    "dataset = load_dataset(\"json\", data_files='./invoice_ner_dataset_testing.jsonl')['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885733db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id', 'file_name'],\n",
       "    num_rows: 1515\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfc0a2",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385c1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_valid = tmp['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = train_valid['train']\n",
    "valid_dataset = train_valid['test']    \n",
    "test_dataset  = tmp['test']     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5d8399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id', 'file_name'],\n",
       "    num_rows: 969\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2082d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(l for row in train_dataset['ner_tags'] for l in row)\n",
    "label_list = sorted(list(unique_labels))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464f3236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CLIENT_ADDRESS': 0,\n",
       " 'B-CLIENT_NAME': 1,\n",
       " 'B-INVOICE_DATE': 2,\n",
       " 'B-INVOICE_NUMBER': 3,\n",
       " 'B-ITEM_DESC': 4,\n",
       " 'B-NET_WORTH': 5,\n",
       " 'B-PRICE': 6,\n",
       " 'B-QUANTITY': 7,\n",
       " 'B-SELLER_NAME': 8,\n",
       " 'B-TOTAL': 9,\n",
       " 'B-VAT': 10,\n",
       " 'I-CLIENT_ADDRESS': 11,\n",
       " 'I-CLIENT_NAME': 12,\n",
       " 'I-ITEM_DESC': 13,\n",
       " 'I-NET_WORTH': 14,\n",
       " 'I-PRICE': 15,\n",
       " 'I-SELLER_NAME': 16,\n",
       " 'I-TOTAL': 17,\n",
       " 'I-VAT': 18,\n",
       " 'O': 19}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff00e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-CLIENT_ADDRESS',\n",
       " 1: 'B-CLIENT_NAME',\n",
       " 2: 'B-INVOICE_DATE',\n",
       " 3: 'B-INVOICE_NUMBER',\n",
       " 4: 'B-ITEM_DESC',\n",
       " 5: 'B-NET_WORTH',\n",
       " 6: 'B-PRICE',\n",
       " 7: 'B-QUANTITY',\n",
       " 8: 'B-SELLER_NAME',\n",
       " 9: 'B-TOTAL',\n",
       " 10: 'B-VAT',\n",
       " 11: 'I-CLIENT_ADDRESS',\n",
       " 12: 'I-CLIENT_NAME',\n",
       " 13: 'I-ITEM_DESC',\n",
       " 14: 'I-NET_WORTH',\n",
       " 15: 'I-PRICE',\n",
       " 16: 'I-SELLER_NAME',\n",
       " 17: 'I-TOTAL',\n",
       " 18: 'I-VAT',\n",
       " 19: 'O'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c71517",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a07c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google-bert/bert-base-cased'\n",
    "# MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658b8852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df084821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = TOKENIZER(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(label2id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                if word_idx < len(label):\n",
    "                    current_label = label[word_idx]\n",
    "                    label_ids.append(label2id[current_label] if current_label.startswith(\"I-\") else -100)\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b96a93b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e3eed45dce4bb88146fae86348b425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_valid = valid_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test  = test_dataset.map(tokenize_and_align_labels,  batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4303be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e88b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "                  for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be01a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=TOKENIZER)\n",
    "\n",
    "def hyperparameter_tuning_optuna(tokenized_train, tokenized_valid, n_trials=20):\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
    "        num_epochs = trial.suggest_int(\"num_train_epochs\", 2, 6)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "        warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.3)\n",
    "        \n",
    "        \n",
    "        def model_init():\n",
    "            return AutoModelForTokenClassification.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    num_labels=len(label_list),\n",
    "                    id2label=id2label,\n",
    "                    label2id=label2id,\n",
    "                    ignore_mismatched_sizes=True\n",
    "            )\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./tmp_trial_{trial.number}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,  \n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            logging_dir=None,  \n",
    "            report_to=None,   \n",
    "            dataloader_pin_memory=False  \n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model_init=model_init,   \n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_valid,\n",
    "            tokenizer=TOKENIZER,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        return eval_result[\"eval_f1\"]\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"OPTUNA RESULTS\")\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best F1 score: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e7a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:03:24,037] A new study created in memory with name: no-name-665ab870-36b2-4ea8-8ba3-dafb3ac8befd\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.083897</td>\n",
       "      <td>0.800278</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.833484</td>\n",
       "      <td>0.973428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.066595</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.938752</td>\n",
       "      <td>0.901270</td>\n",
       "      <td>0.978967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055314</td>\n",
       "      <td>0.892086</td>\n",
       "      <td>0.937618</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.981883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:04:43,089] Trial 0 finished with value: 0.9142857142857144 and parameters: {'learning_rate': 0.00018169775059983996, 'per_device_train_batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 0.07436348443701805, 'warmup_ratio': 0.01221689575246696}. Best is trial 0 with value: 0.9142857142857144.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114467</td>\n",
       "      <td>0.776311</td>\n",
       "      <td>0.844991</td>\n",
       "      <td>0.809196</td>\n",
       "      <td>0.968008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>0.862566</td>\n",
       "      <td>0.927788</td>\n",
       "      <td>0.893989</td>\n",
       "      <td>0.977906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.064045</td>\n",
       "      <td>0.877790</td>\n",
       "      <td>0.936862</td>\n",
       "      <td>0.906364</td>\n",
       "      <td>0.979320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:06:00,229] Trial 1 finished with value: 0.9063643013899049 and parameters: {'learning_rate': 5.456526544696671e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 0.28679051327152755, 'warmup_ratio': 0.07144776535138031}. Best is trial 0 with value: 0.9142857142857144.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [610/610 02:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.134711</td>\n",
       "      <td>0.764072</td>\n",
       "      <td>0.831380</td>\n",
       "      <td>0.796306</td>\n",
       "      <td>0.964326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.077934</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.934972</td>\n",
       "      <td>0.899273</td>\n",
       "      <td>0.976286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.065623</td>\n",
       "      <td>0.881551</td>\n",
       "      <td>0.928544</td>\n",
       "      <td>0.904437</td>\n",
       "      <td>0.977818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.062210</td>\n",
       "      <td>0.884533</td>\n",
       "      <td>0.938374</td>\n",
       "      <td>0.910659</td>\n",
       "      <td>0.979143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.896626</td>\n",
       "      <td>0.944423</td>\n",
       "      <td>0.919904</td>\n",
       "      <td>0.980410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:08:11,337] Trial 2 finished with value: 0.9199042533603388 and parameters: {'learning_rate': 1.969979451014911e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 5, 'weight_decay': 0.008878432408507374, 'warmup_ratio': 0.13454680728578822}. Best is trial 2 with value: 0.9199042533603388.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 01:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>0.854973</td>\n",
       "      <td>0.893762</td>\n",
       "      <td>0.873937</td>\n",
       "      <td>0.975726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055108</td>\n",
       "      <td>0.893841</td>\n",
       "      <td>0.932703</td>\n",
       "      <td>0.912858</td>\n",
       "      <td>0.981795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.896639</td>\n",
       "      <td>0.937996</td>\n",
       "      <td>0.916851</td>\n",
       "      <td>0.982767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:09:31,023] Trial 3 finished with value: 0.9168514412416852 and parameters: {'learning_rate': 0.00011195674393457663, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.21726475899385367, 'warmup_ratio': 0.1541574751665541}. Best is trial 2 with value: 0.9199042533603388.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/122 00:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.101441</td>\n",
       "      <td>0.834176</td>\n",
       "      <td>0.872968</td>\n",
       "      <td>0.853131</td>\n",
       "      <td>0.970571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>0.886249</td>\n",
       "      <td>0.930813</td>\n",
       "      <td>0.907985</td>\n",
       "      <td>0.980911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:10:24,467] Trial 4 finished with value: 0.9079845104185875 and parameters: {'learning_rate': 0.0003003457439007372, 'per_device_train_batch_size': 16, 'num_train_epochs': 2, 'weight_decay': 0.036176343885972115, 'warmup_ratio': 0.2941792596084894}. Best is trial 2 with value: 0.9199042533603388.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 00:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114137</td>\n",
       "      <td>0.737833</td>\n",
       "      <td>0.859735</td>\n",
       "      <td>0.794133</td>\n",
       "      <td>0.965180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.065950</td>\n",
       "      <td>0.854844</td>\n",
       "      <td>0.924008</td>\n",
       "      <td>0.888081</td>\n",
       "      <td>0.978760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:11:18,779] Trial 5 finished with value: 0.8880813953488372 and parameters: {'learning_rate': 0.00046339243553897546, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.18423015753254587, 'warmup_ratio': 0.07195445776889865}. Best is trial 2 with value: 0.9199042533603388.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [610/610 02:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.746154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.741903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.741420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2025-09-12 20:13:27,926] Trial 6 finished with value: 0.0 and parameters: {'learning_rate': 0.0006389428212558982, 'per_device_train_batch_size': 8, 'num_train_epochs': 5, 'weight_decay': 0.08700273744842954, 'warmup_ratio': 0.00022478834174675109}. Best is trial 2 with value: 0.9199042533603388.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 02:30, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091339</td>\n",
       "      <td>0.825760</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>0.867552</td>\n",
       "      <td>0.972397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.056501</td>\n",
       "      <td>0.893563</td>\n",
       "      <td>0.923629</td>\n",
       "      <td>0.908347</td>\n",
       "      <td>0.981471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.872616</td>\n",
       "      <td>0.916824</td>\n",
       "      <td>0.894174</td>\n",
       "      <td>0.979909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>0.903238</td>\n",
       "      <td>0.938752</td>\n",
       "      <td>0.920653</td>\n",
       "      <td>0.982855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.054379</td>\n",
       "      <td>0.898041</td>\n",
       "      <td>0.935728</td>\n",
       "      <td>0.916497</td>\n",
       "      <td>0.983032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.904090</td>\n",
       "      <td>0.944423</td>\n",
       "      <td>0.923817</td>\n",
       "      <td>0.983768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:16:01,653] Trial 7 finished with value: 0.9238165680473372 and parameters: {'learning_rate': 0.00010331202260173673, 'per_device_train_batch_size': 8, 'num_train_epochs': 6, 'weight_decay': 0.06379565292976189, 'warmup_ratio': 0.1784694315218944}. Best is trial 7 with value: 0.9238165680473372.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 01:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249958</td>\n",
       "      <td>0.609848</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.609272</td>\n",
       "      <td>0.933954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.125302</td>\n",
       "      <td>0.793656</td>\n",
       "      <td>0.879773</td>\n",
       "      <td>0.834499</td>\n",
       "      <td>0.968538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>0.814208</td>\n",
       "      <td>0.901323</td>\n",
       "      <td>0.855554</td>\n",
       "      <td>0.971720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:17:18,161] Trial 8 finished with value: 0.855553561815898 and parameters: {'learning_rate': 5.2050515856247006e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3, 'weight_decay': 0.011932147629118229, 'warmup_ratio': 0.1401146494663357}. Best is trial 7 with value: 0.9238165680473372.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/759261500.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 02:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114986</td>\n",
       "      <td>0.732091</td>\n",
       "      <td>0.861626</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.965858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.102333</td>\n",
       "      <td>0.802847</td>\n",
       "      <td>0.852930</td>\n",
       "      <td>0.827131</td>\n",
       "      <td>0.969010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>0.842365</td>\n",
       "      <td>0.905104</td>\n",
       "      <td>0.872608</td>\n",
       "      <td>0.977287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>0.917958</td>\n",
       "      <td>0.889377</td>\n",
       "      <td>0.978584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060571</td>\n",
       "      <td>0.890011</td>\n",
       "      <td>0.933081</td>\n",
       "      <td>0.911037</td>\n",
       "      <td>0.981176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 20:19:25,233] Trial 9 finished with value: 0.9110372831303064 and parameters: {'learning_rate': 0.0005666264161156186, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.14948176255717752, 'warmup_ratio': 0.2532284593470268}. Best is trial 7 with value: 0.9238165680473372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA RESULTS\n",
      "Best trial: 7\n",
      "Best F1 score: 0.9238\n",
      "Best params: {'learning_rate': 0.00010331202260173673, 'per_device_train_batch_size': 8, 'num_train_epochs': 6, 'weight_decay': 0.06379565292976189, 'warmup_ratio': 0.1784694315218944}\n"
     ]
    }
   ],
   "source": [
    "best_params_optuna, study = hyperparameter_tuning_optuna(\n",
    "    tokenized_train, tokenized_valid, n_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a6a844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1664/1805029775.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 02:35, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091339</td>\n",
       "      <td>0.825760</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>0.867552</td>\n",
       "      <td>0.972397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.056636</td>\n",
       "      <td>0.893207</td>\n",
       "      <td>0.929679</td>\n",
       "      <td>0.911078</td>\n",
       "      <td>0.981854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.058019</td>\n",
       "      <td>0.880419</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.900425</td>\n",
       "      <td>0.979762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.052460</td>\n",
       "      <td>0.903156</td>\n",
       "      <td>0.941399</td>\n",
       "      <td>0.921881</td>\n",
       "      <td>0.983444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.052422</td>\n",
       "      <td>0.903994</td>\n",
       "      <td>0.932703</td>\n",
       "      <td>0.918124</td>\n",
       "      <td>0.983474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.906023</td>\n",
       "      <td>0.944045</td>\n",
       "      <td>0.924644</td>\n",
       "      <td>0.983798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=732, training_loss=0.13221080511645542, metrics={'train_runtime': 155.8002, 'train_samples_per_second': 37.317, 'train_steps_per_second': 4.698, 'total_flos': 1266637985385600.0, 'train_loss': 0.13221080511645542, 'epoch': 6.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_ratio=best_params[\"warmup_ratio\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d3f07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID: {'eval_loss': 0.05190393328666687, 'eval_precision': 0.9060232220609579, 'eval_recall': 0.9440453686200379, 'eval_f1': 0.924643584521385, 'eval_accuracy': 0.9837977965003241, 'eval_runtime': 1.9043, 'eval_samples_per_second': 127.606, 'eval_steps_per_second': 16.279, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "metrics_valid = trainer.evaluate()\n",
    "print(\"VALID:\", metrics_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7ae696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST : {'eval_loss': 0.05229390785098076, 'eval_precision': 0.9072615923009624, 'eval_recall': 0.9432989690721649, 'eval_f1': 0.924929389029285, 'eval_accuracy': 0.9839322369818088, 'eval_runtime': 2.4446, 'eval_samples_per_second': 123.948, 'eval_steps_per_second': 15.545, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "metrics_test = trainer.evaluate(eval_dataset=tokenized_test)   \n",
    "print(\"TEST :\", metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2408b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ner_model_best_last/tokenizer_config.json',\n",
       " './ner_model_best_last/special_tokens_map.json',\n",
       " './ner_model_best_last/vocab.txt',\n",
       " './ner_model_best_last/added_tokens.json',\n",
       " './ner_model_best_last/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./ner_model_best_last\")\n",
    "TOKENIZER.save_pretrained(\"./ner_model_best_last\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
